#!/usr/bin/env ruby

require 'optparse'
require 'kafka'
require 'diplomat'
require 'yaml'
require 'hash_symbolizer'
require 'schash'
require 'rb-inotify'
require 'timers'
require 'socket'
require 'fileutils'
require 'logger'
require 'mixlib/shellout'

$stdout.sync = true

Thread.abort_on_exception = true

@config = nil

loglevels = {
  :debug => Logger::DEBUG,
  :info => Logger::INFO,
  :warn => Logger::WARN,
  :error => Logger::Error,
  :fatal => Logger::FATAL,
  :unknown => Logger::UNKNOWN
}

@loglevel = Logger::INFO

opts = OptionParser.new
opts.banner = "Usage: #{$0} [options]"
opts.on( '--config PATH', String, 'Path to settings config' ) { |c| @config = c }
opts.on( '--log-level [LEVEL]', [:debug, :info, :warn, :error, :fatal, :unknown] ) { |l| @loglevel = loglevels[l] }
opts.on( '-h', '--help', 'Display this screen' ) { puts opts; exit 0 }
opts.parse!

unless @config
  puts opts
  exit 1
end

@logger = Logger.new(STDOUT)

@settings = YAML.load_file(@config).symbolize_keys(true)

validator = Schash::Validator.new do
  {
    tailf: {
      files: array_of({
        topic: string,
        prefix: string,
        suffix: optional(string),
        time_pattern: string,
      }),
      position_file: string,
      flush_interval: optional(integer),
      max_batch_lines: optional(integer),
      max_batches: optional(integer),
      from_begining: boolean,
      delete_old_tailed_files: optional(boolean),
      post_delete_command: optional(string),
    },
    kafka: {
      brokers: optional(string),
      consul_service: optional(string),
      consul_tag: optional(string),
      connect_timeout: optional(integer),
      socket_timeout: optional(integer),
      produce: optional(boolean),
    }
  }
end

unless validator.validate(@settings).empty?
  @logger.error("ERROR: bad settings")
  @logger.error(validator.validate(@settings))
  exit 1
end

@settings[:tailf][:files] = @settings[:tailf][:files].map{|h| h.symbolize_keys(true)}

@mutex = Mutex.new

@create_notifier = INotify::Notifier.new
@delete_notifier = INotify::Notifier.new
@tailf_notifier = INotify::Notifier.new

@dirs = {}
@files = {}
@threads = {}
@position_file = @settings[:tailf][:position_file]
@flush_interval = @settings[:tailf].has_key?(:flush_interval) ? @settings[:tailf][:flush_interval] : 1
@max_batch_lines = @settings[:tailf].has_key?(:max_batch_lines) ? @settings[:tailf][:max_batch_lines] : 1024
@max_batches = @settings[:tailf].has_key?(:max_batches) ? @settings[:tailf][:max_batches] : 10
@from_begining = @settings[:tailf][:from_begining]
@delete_old_tailed_files = @settings[:tailf].has_key?(:delete_old_tailed_files) ? @settings[:tailf][:delete_old_tailed_files] : false
@produce = @settings[:kafka].has_key?(:produce) ? @settings[:kafka][:produce] : true

def kafka_connect
  if @settings[:kafka].has_key?(:brokers)
    kafka_nodes = @settings[:kafka][:brokers]
  else
    kafka_nodes = Diplomat::Service.get(@settings[:kafka][:consul_service], :all, {:tag => @settings[:kafka][:consul_tag]}).inject([]) {|hosts,h| hosts << "#{h.Node}:#{h.ServicePort}"; hosts}
  end
  @kafka = Kafka.new(
    seed_brokers: kafka_nodes,
    logger: @logger,
    connect_timeout: @settings[:kafka].has_key?(:socket_timeout) ? @settings[:kafka][:socket_timeout] : 10,
    socket_timeout: @settings[:kafka].has_key?(:socket_timeout) ? @settings[:kafka][:socket_timeout] : 60
  )
  @producer = @kafka.producer(
    required_acks: 0,
    max_retries: 2,
    retry_backoff: 1,
    compression_codec: :snappy,
    max_buffer_size: @max_batch_lines + 1
  )
end

def write_position_file
  @mutex.synchronize do
    File.open(@position_file, 'w') do |file|
      @files.each do |path, attrs|
        file.puts "#{path} #{attrs[:pattern]} #{attrs[:topic]} #{attrs[:inode]} #{attrs[:offset]}"
      end
    end
  end
end

def load_position_file
  if File.exist?(@position_file)
    IO.readlines(@position_file).each do |line|
      path, pattern, topic, inode, offset = line.split(' ')
      #Load state only for that exist with same inode and were not truncated/rewinded.
      if File.exists?(path) and File.stat(path).ino == inode.to_i and File.stat(path).size >= offset.to_i
        @files[path] = { :pattern => pattern, :topic => topic, :inode => inode.to_i, :offset => offset.to_i }
      end
    end
  end
  write_position_file
end

load_position_file

@topics = @settings[:tailf][:files].map{|tailf_file| tailf_file[:topic]}

kafka_connect if @produce

@producer_queue = SizedQueue.new(@max_batches * 10)

@producer_thread = Thread.new do
  loop do
    batch = @producer_queue.pop
    if @produce
      begin
        batch[:messages].each do |msg|
          @producer.produce(msg, topic: batch[:topic])
        end
        @producer.deliver_messages
      rescue Kafka::DeliveryFailed
        @logger.warn("Producer failed to deliver messages to brokers, retrying in 1 second ...")
        sleep 1
        retry
      end
    end
    @files[batch[:path]][:offset] = batch[:offset]
  end
end

def kafka_produce(path, buffer, offset)
  truncated = nil

  messages = []
  while msg = buffer.shift
    unless msg[-1] == "\n"
      if buffer.empty?
        truncated = msg
      else
        msg = msg + buffer.shift
        messages << msg.strip
      end
    else
      messages << msg.strip
    end
  end
  @producer_queue.push({ :topic => @files[path][:topic], :path => path, :messages => messages, :offset => offset })

  truncated
end

def tailf(path)
  file = File.open(path, 'r')
  @files[path][:fd] = file
  file.seek(@files[path][:offset], IO::SEEK_SET)

  truncated = nil
  loop do #Fast read file in batches until we reach EOF upon which we start the tailf modify watcher
    batch = file.each_line.take(@max_batch_lines)
    break if batch.empty?
    batch.unshift(truncated + batch.shift) if truncated
    truncated = kafka_produce(path, batch, file.pos)
  end

  mutex = Mutex.new
  @tailf_notifier.watch(path, :modify) do |event|
    mutex.synchronize do
      unless file.closed?
        (1..@max_batches).each do |i|
          batch = file.each_line.take(@max_batch_lines)
          break if batch.empty?
          batch.unshift(truncated + batch.shift) if truncated
          truncated = kafka_produce(path, batch, file.pos)
        end
      else
        @logger.warn("watcher got modify event on closed file #{event.name}")
      end
    end
  end
end

@time_regexp_hash = {
  'Y' => '[0-9]{4}',
  'm' => '[0-9]{2}',
  'd' => '[0-9]{2}',
  'H' => '[0-9]{2}',
  'M' => '[0-9]{2}'
}

def time_pattern_to_regexp(pattern)
  pattern.gsub(/%([^%])/) do
    match = $1
    @time_regexp_hash.has_key?(match) ? @time_regexp_hash[match] : match
  end
end

#Scan existing files that match watched prefixes and start failing them
@settings[:tailf][:files].each do |tailf_file|
  tailf_file[:prefix] = File.expand_path(tailf_file[:prefix])
  dir = File.dirname(tailf_file[:prefix])
  if File.exists?(dir) and File.directory?(dir)
    @dirs[dir] ||= []
    @dirs[dir] << { :prefix => File.basename(tailf_file[:prefix]), :pattern => tailf_file[:time_pattern], :suffix => "#{tailf_file[:suffix]}", :topic => tailf_file[:topic]}
    Dir.glob("#{tailf_file[:prefix]}*#{tailf_file[:suffix]}").each do |path|
      if path.match(Regexp.new(time_pattern_to_regexp(tailf_file[:time_pattern])))
        unless File.directory?(path)
          #Populate state only if it was not loaded from position file
          unless @files.has_key?(path)
            @files[path] = { :pattern => tailf_file[:time_pattern], :topic => tailf_file[:topic], :inode => File.stat(path).ino, :offset => 0 }
            @files[path][:offset] = File.stat(path).size unless @from_begining
          end
          @threads[path] = Thread.new { tailf(path) } unless @threads.has_key?(path)
        end
      end
    end
  end
end

def delete_old_tailed_files
  @mutex.synchronize do
    @files.each_key do |path|
      unless path.match(Regexp.new(Time.now.strftime(@files[path][:pattern])))
        if File.exists?(path) and File.stat(path).ino == @files[path][:inode] and File.stat(path).size == @files[path][:offset] and (Time.now - File.stat(path).mtime) > 30
          @logger.info("Deleteing old time pattern fully kafka produced file #{path}")
          FileUtils.rm_r(path)
          if @settings[:tailf].has_key?(:post_delete_command)
            @logger.info("Running post delete command => #{@settings[:tailf][:post_delete_command]}")
            command = Mixlib::ShellOut.new(@settings[:tailf][:post_delete_command])
            begin
              command.run_command
              if command.error?
                @logger.error("Failed post delete command => #{@settings[:tailf][:post_delete_command]}")
                @logger.info("STDOUT: #{command.stdout}")
                @logger.info("STDERR: #{command.stderr}")
              end
            rescue => e
              @logger.error("Failed post delete command => #{@settings[:tailf][:post_delete_command]}")
              @logger.info(e.message)
            end
          end
        end
      end
    end
  end
end

@timers = Timers::Group.new
@uploads_timer = @timers.every(@flush_interval) { write_position_file }
@delete_old_tailed_files_timer = @timers.every(60) { delete_old_tailed_files } if @delete_old_tailed_files
Thread.new { loop { @timers.wait } }

@dirs.each_key do |dir|

  @create_notifier.watch(dir, :create, :moved_to) do |event|
    @mutex.synchronize do
      path = "#{dir}/#{event.name}"
      match = @dirs[dir].detect{|h| event.name.match(Regexp.new(h[:prefix] + time_pattern_to_regexp(h[:pattern]) + h[:suffix]))}
      if match
        unless File.directory?(path)
          unless @threads.has_key?(path)
            @logger.info("File #{event.name} was created in / moved into watched dir #{dir}")
            @files[path] = { :pattern => match[:pattern], :topic => match[:topic], :inode => File.stat(path).ino, :offset => 0 }
            @threads[path] = Thread.new { tailf(path) }
          end
        end
      end
    end
  end

  @delete_notifier.watch(dir, :delete, :moved_from) do |event|
    @mutex.synchronize do
      path = "#{dir}/#{event.name}"
      if @threads.has_key?(path)
        @logger.info("File #{event.name} was deleted / moved from watched dir #{dir}")
        if @threads[path].alive?
          @threads[path].terminate
          @threads[path].join
        end
        @threads.delete(path)
        @files[path][:fd].close unless @files[path][:fd].closed?
        @files.delete(path)
      end
    end
  end

end

Thread.new { @create_notifier.run }
Thread.new { @delete_notifier.run }

@tailf_notifier.run
